{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c96c41ab-f018-458b-87c1-4e6c2b98d83a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/enemy/miniconda3/envs/mmir/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of Wav2Vec2ConformerModel were not initialized from the model checkpoint at facebook/wav2vec2-conformer-rope-large-960h-ft and are newly initialized: ['wav2vec2_conformer.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2_conformer.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/enemy/miniconda3/envs/mmir/lib/python3.11/site-packages/torch/nn/modules/conv.py:306: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at /opt/conda/conda-bld/pytorch_1712608853085/work/aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
      "  return F.conv1d(input, weight, bias, self.stride,\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "import os, sys\n",
    "\n",
    "currentUrl = os.path.dirname(\"./notebooks\")\n",
    "parentUrl = os.path.abspath(os.path.join(currentUrl, os.pardir))\n",
    "sys.path.append(parentUrl)\n",
    "\n",
    "from src.models.MultiModalFusion import MultiModalFusion\n",
    "from src.trainer.MultiModalFusionTrainer import MultiModalFusionTrainer\n",
    "from src.utils.Retrieval import FetchSimilar\n",
    "import yaml\n",
    "from IPython.display import Audio, display, display_jpeg, Image\n",
    "from IPython.core.display import HTML\n",
    "from PIL import Image\n",
    "\n",
    "with open('../configs/MultiModalFusion.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "fetcher = FetchSimilar(\n",
    "    chkpt_path=\"../logs/MultiModalFusion/yd2gaqhs/checkpoints/epoch=31-val_loss=2.23-val_mean_similarity=0.37.ckpt\",\n",
    "    image_path=\"../datasets/speech-handsign_commands_balanced2/handsign/\",\n",
    "    audio_path=\"../datasets/speech-handsign_commands_balanced2/speech/\",\n",
    "    device=\"cuda\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f55d589-864a-4047-b468-61b3419afc1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_top_k(query_path:str|pathlib.PosixPath, top_k:dict, query_class:str|None=None) -> None:\n",
    "    data = []\n",
    "    if type(query_path) == str:\n",
    "        query_path = pathlib.Path(query_path)\n",
    "    if query_path.suffix == \".wav\":\n",
    "        query_html = f'<audio controls src=\"{query_path}\" style=\"display:block; margin:0 auto;\"></audio>'\n",
    "    else:\n",
    "        query_html = f'<img src=\"{query_path}\" alt=\"query image\" style=\"max-width:300px; height:auto; display:block; margin:0 auto;\">'\n",
    "        \n",
    "    for k, v in top_k.items():\n",
    "        path, cls, embed, score = v.values()\n",
    "        data += [{ \"path\": path, \"cls\": cls, \"embed\": embed, \"score\": score, \"modality\": k.split(\"#\")[1]}]\n",
    "    # Define a function to render HTML for images and audio\n",
    "    def render_table(idx, row):\n",
    "        if row[\"modality\"] == \"image\":\n",
    "            display = f'<img src=\"{row[\"path\"]}\" alt=\"Image\" style=\"width:100px;height:auto;\">'\n",
    "        else:\n",
    "            display = f'<audio controls src=\"{row[\"path\"]}\" style=\"width:200px;\"></audio>'\n",
    "        return f'<tr><td>{idx+1}</td><td>{row[\"cls\"]}</td><td>{display}</td><td>{row[\"score\"]:.3f}</td></tr>'\n",
    "    \n",
    "    # Generate the table HTML\n",
    "    table_html = \"\"\"\n",
    "    <table border=\"1\" style=\"border-collapse:collapse; text-align:center; margin:auto;\">\n",
    "        <tr>\n",
    "            <th>Rank</th>\n",
    "            <th>Class</th>\n",
    "            <th>Display</th>\n",
    "            <th>Similarity Score</th>\n",
    "        </tr>\n",
    "    \"\"\"\n",
    "    for idx, row in enumerate(data):\n",
    "        table_html += render_table(idx, row)\n",
    "    table_html += \"</table>\"\n",
    "\n",
    "    # Combine top media and the table\n",
    "    full_html = f\"\"\"\n",
    "    <div style=\"text-align:center; margin-bottom:20px;\">\n",
    "        <b>{query_class if query_class is not None else str(query_path)}</b>\n",
    "        {query_html}\n",
    "    </div>\n",
    "    {table_html}\n",
    "    \"\"\"\n",
    "    \n",
    "    # Display the complete HTML\n",
    "    display(HTML(full_html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63af7c0e-8839-47e1-9aea-eafe25a998bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div style=\"text-align:center; margin-bottom:20px;\">\n",
       "        <b>no</b>\n",
       "        <audio controls src=\"../datasets/speech-handsign_commands_balanced2/speech/no/no_11.wav\" style=\"display:block; margin:0 auto;\"></audio>\n",
       "    </div>\n",
       "    \n",
       "    <table border=\"1\" style=\"border-collapse:collapse; text-align:center; margin:auto;\">\n",
       "        <tr>\n",
       "            <th>Rank</th>\n",
       "            <th>Class</th>\n",
       "            <th>Display</th>\n",
       "            <th>Similarity Score</th>\n",
       "        </tr>\n",
       "    <tr><td>1</td><td>no</td><td><img src=\"../datasets/speech-handsign_commands_balanced2/handsign/no/no_58.jpeg\" alt=\"Image\" style=\"width:100px;height:auto;\"></td><td>0.159</td></tr><tr><td>2</td><td>no</td><td><img src=\"../datasets/speech-handsign_commands_balanced2/handsign/no/no_6.jpeg\" alt=\"Image\" style=\"width:100px;height:auto;\"></td><td>0.154</td></tr><tr><td>3</td><td>no</td><td><img src=\"../datasets/speech-handsign_commands_balanced2/handsign/no/no_9.jpeg\" alt=\"Image\" style=\"width:100px;height:auto;\"></td><td>0.153</td></tr><tr><td>4</td><td>no</td><td><img src=\"../datasets/speech-handsign_commands_balanced2/handsign/no/no_14.jpeg\" alt=\"Image\" style=\"width:100px;height:auto;\"></td><td>0.151</td></tr><tr><td>5</td><td>no</td><td><img src=\"../datasets/speech-handsign_commands_balanced2/handsign/no/no_2.jpeg\" alt=\"Image\" style=\"width:100px;height:auto;\"></td><td>0.151</td></tr><tr><td>6</td><td>no</td><td><img src=\"../datasets/speech-handsign_commands_balanced2/handsign/no/no_60.jpeg\" alt=\"Image\" style=\"width:100px;height:auto;\"></td><td>0.150</td></tr><tr><td>7</td><td>no</td><td><img src=\"../datasets/speech-handsign_commands_balanced2/handsign/no/no_56.jpeg\" alt=\"Image\" style=\"width:100px;height:auto;\"></td><td>0.150</td></tr><tr><td>8</td><td>no</td><td><img src=\"../datasets/speech-handsign_commands_balanced2/handsign/no/no_70.jpeg\" alt=\"Image\" style=\"width:100px;height:auto;\"></td><td>0.150</td></tr><tr><td>9</td><td>no</td><td><img src=\"../datasets/speech-handsign_commands_balanced2/handsign/no/no_30.jpeg\" alt=\"Image\" style=\"width:100px;height:auto;\"></td><td>0.150</td></tr><tr><td>10</td><td>no</td><td><img src=\"../datasets/speech-handsign_commands_balanced2/handsign/no/no_12.jpeg\" alt=\"Image\" style=\"width:100px;height:auto;\"></td><td>0.150</td></tr></table>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query_path = \"../datasets/speech-handsign_commands_balanced2/speech/no/no_11.wav\"\n",
    "top_k, query_info = fetcher.top_k(\n",
    "    path=query_path,\n",
    "    modality=\"image\",\n",
    "    k=10\n",
    ")\n",
    "render_top_k(query_path, top_k, \"no\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ebf2ad2-fdc5-47ae-b141-41e848adb5dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div style=\"text-align:center; margin-bottom:20px;\">\n",
       "        <b>stop</b>\n",
       "        <img src=\"../datasets/test_stop2.jpeg\" alt=\"query image\" style=\"max-width:300px; height:auto; display:block; margin:0 auto;\">\n",
       "    </div>\n",
       "    \n",
       "    <table border=\"1\" style=\"border-collapse:collapse; text-align:center; margin:auto;\">\n",
       "        <tr>\n",
       "            <th>Rank</th>\n",
       "            <th>Class</th>\n",
       "            <th>Display</th>\n",
       "            <th>Similarity Score</th>\n",
       "        </tr>\n",
       "    <tr><td>1</td><td>stop</td><td><audio controls src=\"../datasets/speech-handsign_commands_balanced2/speech/stop/stop_12.wav\" style=\"width:200px;\"></audio></td><td>0.147</td></tr><tr><td>2</td><td>stop</td><td><audio controls src=\"../datasets/speech-handsign_commands_balanced2/speech/stop/stop_53.wav\" style=\"width:200px;\"></audio></td><td>0.146</td></tr><tr><td>3</td><td>yes</td><td><audio controls src=\"../datasets/speech-handsign_commands_balanced2/speech/yes/yes_47.wav\" style=\"width:200px;\"></audio></td><td>0.142</td></tr><tr><td>4</td><td>left</td><td><audio controls src=\"../datasets/speech-handsign_commands_balanced2/speech/left/left_50.wav\" style=\"width:200px;\"></audio></td><td>0.142</td></tr><tr><td>5</td><td>stop</td><td><audio controls src=\"../datasets/speech-handsign_commands_balanced2/speech/stop/stop_19.wav\" style=\"width:200px;\"></audio></td><td>0.135</td></tr><tr><td>6</td><td>no</td><td><audio controls src=\"../datasets/speech-handsign_commands_balanced2/speech/no/no_47.wav\" style=\"width:200px;\"></audio></td><td>0.135</td></tr><tr><td>7</td><td>stop</td><td><audio controls src=\"../datasets/speech-handsign_commands_balanced2/speech/stop/stop_42.wav\" style=\"width:200px;\"></audio></td><td>0.133</td></tr><tr><td>8</td><td>yes</td><td><audio controls src=\"../datasets/speech-handsign_commands_balanced2/speech/yes/yes_58.wav\" style=\"width:200px;\"></audio></td><td>0.132</td></tr><tr><td>9</td><td>stop</td><td><audio controls src=\"../datasets/speech-handsign_commands_balanced2/speech/stop/stop_68.wav\" style=\"width:200px;\"></audio></td><td>0.131</td></tr><tr><td>10</td><td>stop</td><td><audio controls src=\"../datasets/speech-handsign_commands_balanced2/speech/stop/stop_61.wav\" style=\"width:200px;\"></audio></td><td>0.129</td></tr></table>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query_path = \"../datasets/test_stop2.jpeg\"\n",
    "top_k, query_info = fetcher.top_k(\n",
    "    path=query_path,\n",
    "    modality=\"audio\",\n",
    "    k=10\n",
    ")\n",
    "render_top_k(query_path, top_k, \"stop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a9301c0-7ed7-44f3-8ca7-b0fe4755da8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div style=\"text-align:center; margin-bottom:20px;\">\n",
       "        <b>stop</b>\n",
       "        <audio controls src=\"../datasets/test_stop.wav\" style=\"display:block; margin:0 auto;\"></audio>\n",
       "    </div>\n",
       "    \n",
       "    <table border=\"1\" style=\"border-collapse:collapse; text-align:center; margin:auto;\">\n",
       "        <tr>\n",
       "            <th>Rank</th>\n",
       "            <th>Class</th>\n",
       "            <th>Display</th>\n",
       "            <th>Similarity Score</th>\n",
       "        </tr>\n",
       "    <tr><td>1</td><td>stop</td><td><img src=\"../datasets/speech-handsign_commands_balanced2/handsign/stop/stop_57.jpeg\" alt=\"Image\" style=\"width:100px;height:auto;\"></td><td>0.313</td></tr><tr><td>2</td><td>stop</td><td><img src=\"../datasets/speech-handsign_commands_balanced2/handsign/stop/stop_58.jpeg\" alt=\"Image\" style=\"width:100px;height:auto;\"></td><td>0.312</td></tr><tr><td>3</td><td>stop</td><td><img src=\"../datasets/speech-handsign_commands_balanced2/handsign/stop/stop_25.jpeg\" alt=\"Image\" style=\"width:100px;height:auto;\"></td><td>0.311</td></tr><tr><td>4</td><td>stop</td><td><img src=\"../datasets/speech-handsign_commands_balanced2/handsign/stop/stop_60.jpeg\" alt=\"Image\" style=\"width:100px;height:auto;\"></td><td>0.307</td></tr><tr><td>5</td><td>stop</td><td><img src=\"../datasets/speech-handsign_commands_balanced2/handsign/stop/stop_56.jpeg\" alt=\"Image\" style=\"width:100px;height:auto;\"></td><td>0.304</td></tr><tr><td>6</td><td>stop</td><td><img src=\"../datasets/speech-handsign_commands_balanced2/handsign/stop/stop_10.jpeg\" alt=\"Image\" style=\"width:100px;height:auto;\"></td><td>0.303</td></tr><tr><td>7</td><td>stop</td><td><img src=\"../datasets/speech-handsign_commands_balanced2/handsign/stop/stop_59.jpeg\" alt=\"Image\" style=\"width:100px;height:auto;\"></td><td>0.302</td></tr><tr><td>8</td><td>stop</td><td><img src=\"../datasets/speech-handsign_commands_balanced2/handsign/stop/stop_55.jpeg\" alt=\"Image\" style=\"width:100px;height:auto;\"></td><td>0.298</td></tr><tr><td>9</td><td>stop</td><td><img src=\"../datasets/speech-handsign_commands_balanced2/handsign/stop/stop_62.jpeg\" alt=\"Image\" style=\"width:100px;height:auto;\"></td><td>0.294</td></tr><tr><td>10</td><td>stop</td><td><img src=\"../datasets/speech-handsign_commands_balanced2/handsign/stop/stop_6.jpeg\" alt=\"Image\" style=\"width:100px;height:auto;\"></td><td>0.292</td></tr></table>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query_path = \"../datasets/test_stop.wav\"\n",
    "top_k, query_info = fetcher.top_k(\n",
    "    path=query_path,\n",
    "    modality=\"image\",\n",
    "    k=10\n",
    ")\n",
    "render_top_k(query_path, top_k, \"stop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9c514ea7-3aa6-4684-87ba-4bd53a824cd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ConformerModel were not initialized from the model checkpoint at facebook/wav2vec2-conformer-rope-large-960h-ft and are newly initialized: ['wav2vec2_conformer.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2_conformer.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(-0.0472, device='cuda:0')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model = MultiModalFusion(**config['model_params'])\n",
    "model = MultiModalFusionTrainer.load_from_checkpoint(\"./logs/MultiModalFusion/bmwnmh9z/checkpoints/epoch=35-val_loss=2.18-val_mean_similarity=0.38.ckpt\")\n",
    "model = model.model.to('cuda')\n",
    "sim = nn.CosineSimilarity(dim=-1, eps=1e-6)\n",
    "img_file = './datasets/speech-handsign_commands_balanced/handsign/left/hand1_g_bot_seg_2_cropped.jpeg'\n",
    "wav_file = './datasets/speech-handsign_commands_balanced/speech/go/888a0c49_nohash_2.wav'\n",
    "img_embed = model.encode_image(img_file)\n",
    "audio_embed = model.encode_speech(wav_file)\n",
    "sim(img_embed, audio_embed)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
